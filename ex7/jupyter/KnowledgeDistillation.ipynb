{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e63172-2733-49a5-b501-524dadec579b",
   "metadata": {},
   "source": [
    "## **Notebook Setup: Imports, GPU/CPU Check, and Optimizer Configuration**\n",
    "\n",
    "This notebook cell performs the initial environment setup, including importing required libraries and checking your current hardware environment:\n",
    "\n",
    "- **TensorFlow** and **Keras** are imported for building and training deep learning models.\n",
    "- We define a custom function `get_optimizer_for_platform()` to detect if your system is running on Apple Silicon (M1/M2). TensorFlow has a known issue causing slowdowns on these chips with the default optimizer implementation, so this function automatically selects the legacy Adam optimizer when necessary.\n",
    "- Finally, the cell prints out TensorFlow version information and checks whether a GPU is available. If a GPU is found, it will be used automatically by TensorFlow, greatly speeding up model training. If not, it will default to using your CPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Things to check when running this cell**:\n",
    "\n",
    "- Confirm TensorFlow is properly installed and check the printed version (2.13.0 is used for this exercise)\n",
    "- Verify whether your notebook is correctly detecting the presence (or absence) of a GPU. (Absence of a GPU is not a problem, a CPU is also enough to complete this exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e76fc-b77b-41cd-b1c2-b7834b699c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5451b7e9-8a96-4cb4-ae85-e771040759d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import visualkeras\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import platform\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"visualkeras\")\n",
    "\n",
    "seed = 142\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "#tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def get_optimizer_for_platform(learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Returns legacy Adam if running on Apple Silicon (M1/M2)\n",
    "    to avoid known slowdowns in TF 2.11+,\n",
    "    otherwise returns the standard Adam optimizer.\n",
    "    \"\"\"\n",
    "    # Check if platform is Mac (Darwin) and Apple Silicon (arm64)\n",
    "    is_apple_silicon = (platform.system() == \"Darwin\") and (\"arm64\" in platform.platform())\n",
    "    \n",
    "    if is_apple_silicon:\n",
    "        print(\"Detected Apple Silicon. Using legacy Adam optimizer.\")\n",
    "        return tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
    "    else:\n",
    "        print(\"Using standard Adam optimizer.\")\n",
    "        return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU detected. The following GPU(s) will be used:\")\n",
    "    for idx, gpu in enumerate(gpus):\n",
    "        print(f\"  GPU {idx}: {gpu}\")\n",
    "else:\n",
    "    print(\"No GPU detected. Using CPU instead.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260b802-bfcc-4c59-9d84-9b202ecc677f",
   "metadata": {},
   "source": [
    "###  **Task 1: Model Training**\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "- Load the **CIFAR-10** dataset, which consists of **60,000** 32x32 color images across **10 classes**.\n",
    "- Normalize the image pixel values from the original range `[0, 255]` to `[0, 1]` to help the model learn more efficiently.\n",
    "- Convert labels from integers to **one-hot encoded vectors**, making them suitable for classification with a categorical cross-entropy loss.\n",
    "- Split the original training data into two subsets:\n",
    "  - **Training set**: 90% of the original training data.\n",
    "  - **Validation set**: 10% of the original training data.  \n",
    "  This allows us to monitor performance and prevent overfitting by checking accuracy on data the model hasn't trained on.\n",
    "\n",
    "**Key Points:**\n",
    "- CIFAR-10 has **60,000 color images (32x32 pixels)** in 10 classes, split into:\n",
    "  - 50,000 images for training.\n",
    "  - 10,000 images reserved for testing.\n",
    "- Classes in CIFAR-10 include: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck.\n",
    "\n",
    "---\n",
    "\n",
    "### **Discussion Points:**\n",
    "- Why do we use one-hot encoding for labels?\n",
    "- What is the purpose of creating a validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e838d-401f-4519-b3b6-a57edd5ecfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Scale images to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test  = x_test.astype(\"float32\")  / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test  = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Create a validation set from training data (e.g., 10% validation split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Training data shape:\", x_train.shape)\n",
    "print(\"Validation data shape:\", x_val.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c8806-f1b0-496b-bd04-607c16e867aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# Display some images and their classes\n",
    "# -----------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# CIFAR-10 class names\n",
    "class_names = [\n",
    "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
    "]\n",
    "\n",
    "# Show the first num_images images in x_train\n",
    "num_images = 8\n",
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "for i in range(num_images):\n",
    "    ax = plt.subplot(1, num_images, i+1)\n",
    "    \n",
    "    label_index = np.argmax(y_train[i])\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(class_names[label_index])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf8c7c-f787-48db-93cb-084c300bf143",
   "metadata": {},
   "source": [
    "\n",
    "### **Define the CNN Model for CIFAR-10 Classification**\n",
    "\n",
    "This notebook cell defines a Convolutional Neural Network (CNN) architecture tailored for classifying the CIFAR-10 dataset. The architecture consists of:\n",
    "\n",
    "- **Three convolutional blocks**, each designed to progressively capture more complex features from the input images:\n",
    "  - **Block 1:** Extracts basic features using two `Conv2D` layers with 32 filters, followed by batch normalization and dropout to reduce overfitting.\n",
    "  - **Block 2:** Uses two `Conv2D` layers with 64 filters for deeper feature extraction, again applying batch normalization, pooling, and dropout.\n",
    "  - **Block 3:** Uses two `Conv2D` layers with 128 filters, again with batch normalization.\n",
    "\n",
    "- **Global Average Pooling** (`GlobalAveragePooling2D`): Reduces feature maps to a vector, capturing high-level information across the entire image.\n",
    "\n",
    "- **Fully-connected (dense) layers**:\n",
    "  - A hidden layer with **256 units** and a ReLU activation to refine extracted features.\n",
    "  - A final layer with **10 units** and **softmax activation**, producing a probability distribution across the 10 CIFAR-10 classes.\n",
    "\n",
    "- **Dropout layers**: Regularization technique applied throughout to mitigate **overfitting** by randomly deactivating neurons during training.\n",
    "\n",
    "**Questions to Consider:**\n",
    "- How many total parameters does this model have?\n",
    "- How would changing the number of filters in convolutional layers affect model complexity and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba66bb-bd38-4009-bb26-e4850b6c6ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cifar10_model():\n",
    "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Global Average Pooling + final dense layers\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e446a-eacd-44d7-8ebb-cff0411a0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cifar10_model()\n",
    "model.summary()\n",
    "small_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688cc64-28ca-455e-a0a9-e9df26e25dd4",
   "metadata": {},
   "source": [
    "### **Compile the Model**\n",
    "\n",
    "In this cell, we compile the model, specifying how it will learn from the data:\n",
    "\n",
    "- **Optimizer (`Adam`)**: An efficient and popular optimizer that adapts learning rates for each parameter, often providing fast and stable convergence.\n",
    "- **Loss function (`categorical_crossentropy`)**: Used for multi-class classification tasks when labels are one-hot encoded.\n",
    "- **Metrics (`accuracy`)**: Tracks the fraction of correctly classified examples during training and validation.\n",
    "\n",
    "**Special note for Apple Silicon Macs**:  \n",
    "The provided code automatically selects between TensorFlow’s standard `Adam` optimizer and the legacy version (`tf.keras.optimizers.legacy.Adam`) based on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bd776-d925-4262-9194-b9b29c6b6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=get_optimizer_for_platform(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c2327-c30b-4613-97ce-24e495182fce",
   "metadata": {},
   "source": [
    "### **Model Checkpointing**\n",
    "\n",
    "This cell sets up a **checkpoint callback** for the training process. The callback automatically saves the **best-performing model weights** based on the **highest validation accuracy** (`val_accuracy`). By using this callback:\n",
    "\n",
    "- Only the weights corresponding to the best validation performance are saved (`save_best_only=True`).\n",
    "- The weights are saved in a file named `\"best_model_checkpoint.h5\"`.\n",
    "\n",
    "Later, you can easily restore these weights to perform inference or further training without losing progress.\n",
    "\n",
    "**Points to Consider:**\n",
    "- Why is checkpointing valuable during model training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a38323-5e3f-401a-8e12-aab67a01eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"best_model_checkpoint.h5\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d900648-e107-47c3-8ea9-095581b03b02",
   "metadata": {},
   "source": [
    "\n",
    "### **Model Training**\n",
    "\n",
    "In this cell, we start the training process for the CNN model using the training set:\n",
    "\n",
    "- **Epochs** (`train_epochs = 10`): The entire training dataset will be processed **10 times**, allowing the model to improve gradually.\n",
    "- **Batch Size** (`train_batch_size = 64`): The number of images used in each training step. A batch size of **64** strikes a balance between memory usage and convergence stability.\n",
    "- **Validation Data**: During training, the model's performance is evaluated on the validation set (`x_val`, `y_val`) at the end of each epoch. This helps prevent overfitting and provides an estimate of generalization performance.\n",
    "\n",
    "**After training**, the checkpoint callback saves only the weights from the epoch with the highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3019bf4-0fab-4eb2-bbc7-8af6ad5ecc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs     = 5\n",
    "train_batch_size = 64\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=train_epochs,\n",
    "    batch_size=train_batch_size,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a83aeb-016e-43b3-bea8-eb010041fe52",
   "metadata": {},
   "source": [
    "### **Evaluate the Trained Model Using the Best Checkpoint**\n",
    "\n",
    "In this step, we load the **best model weights** saved during training—those corresponding to the highest validation accuracy—and evaluate the model on the **test dataset**. This allows us to measure how well the model generalizes to unseen data, providing a reliable estimate of real-world performance.\n",
    "\n",
    "**Key Points:**\n",
    "- The best checkpoint was selected based on **highest validation accuracy**.\n",
    "- The evaluation result (test accuracy) gives us the most realistic expectation of the model's actual predictive performance.\n",
    "\n",
    "**Discussion Questions:**\n",
    "- Why do we use the test set rather than the validation set for the final evaluation?\n",
    "- What does a large gap between validation and test accuracies indicate about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560d449-e141-4077-a42c-2846599d9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "loss, small_model_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Best checkpoint TF model test accuracy: {small_model_acc:.4f}\")\n",
    "print(f\"Best checkpoint TF model loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f90873-97e6-4b6f-ad8c-741d8b6f5ef6",
   "metadata": {},
   "source": [
    "### **Task 2: Post-Training Integer Quantization of the Model**\n",
    "\n",
    "In this cell, the previously trained Keras model (`model`) is converted into a **fully quantized TensorFlow Lite model** (`INT8`). This quantization process significantly reduces the model's size and memory footprint, making it suitable for deployment on resource-constrained devices (e.g., microcontrollers or mobile phones).\n",
    "\n",
    "**How this works:**\n",
    "- We use a **representative dataset** (100 images from the training set) for calibration. This step helps TensorFlow Lite determine how to optimally map floating-point values to INT8 representation.\n",
    "- The model parameters and intermediate activations are converted from FP32 (32-bit floats) to INT8 (8-bit integers), greatly reducing the size and improving inference speed at the potential cost of minor accuracy loss.\n",
    "\n",
    "**Output:**\n",
    "- The quantized model is saved to the file: `\"quantized_model.tflite\"`.\n",
    "\n",
    "**Questions for discussion:**\n",
    "- How much size reduction do you expect from quantization?\n",
    "- What impact might quantization have on model accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425bf28-f5e1-4c5d-a621-79051e2adda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-Training Quantization (Full Integer)\n",
    "\n",
    "def representative_data_gen():\n",
    "    # Provide a small subset of training data for calibration\n",
    "    for i in range(100):\n",
    "        yield [x_train[i:i+1]]\n",
    "\n",
    "# Convert Keras model to TFLite with full integer quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "quantized_model_path = \"quantized_model.tflite\"\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"Quantized TFLite model saved to:\", quantized_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334775a-68a1-49d0-859b-c106194ccc23",
   "metadata": {},
   "source": [
    "## **Evaluate the Quantized TFLite Model on CIFAR-10 Test Data**\n",
    "\n",
    "In this cell, we **evaluate the fully integer-quantized (INT8) TFLite model** using the CIFAR-10 test set. The process involves:\n",
    "\n",
    "- Loading the quantized model (`.tflite` file).\n",
    "- Adjusting input images from floating-point values `[0,1]` to the INT8 representation required by the quantized model, using the model's input scale and zero-point.\n",
    "- Running inference using TensorFlow Lite's interpreter to predict class labels for each image.\n",
    "- Calculating accuracy by comparing the predicted labels with the ground-truth labels.\n",
    "\n",
    "This evaluation allows us to compare the quantized model's performance directly with the original (non-quantized) model, providing insights into the trade-off between model size and accuracy.\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- How did accuracy, if any, change after quantization?\n",
    "- What benefits do quantized models have in practical deployment scenarios, such as embedded devices?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbaae21-39e2-4698-99a5-b561a0a3ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Quantized TFLite Model\n",
    "\n",
    "def evaluate_tflite_model(tflite_model_path, x_test, y_test):\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # We'll assume per-tensor quantization\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "\n",
    "    correct = 0\n",
    "    total = x_test.shape[0]\n",
    "\n",
    "    for i in range(total):\n",
    "        # Convert float [0,1] to int8 using scale, zero_point\n",
    "        float_img = x_test[i:i+1]\n",
    "        int8_img = np.round(float_img / input_scale + input_zero_point).astype(np.int8)\n",
    "        interpreter.set_tensor(input_details[0]['index'], int8_img)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        pred_label = np.argmax(output_data[0])\n",
    "        true_label = np.argmax(y_test[i])\n",
    "        if pred_label == true_label:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "small_quantized_acc = evaluate_tflite_model(quantized_model_path, x_test, y_test)\n",
    "print(f\"Quantized TFLite model test accuracy: {small_quantized_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d12aacc-b0cd-430c-b76f-0727e5dcf8af",
   "metadata": {},
   "source": [
    "### **Confusion Matrix Interpretation**\n",
    "\n",
    "This plot shows a **confusion matrix** visualized using a heatmap, indicating the performance of the classification model on the CIFAR-10 test dataset:\n",
    "\n",
    "- **Diagonal cells**: Represent correctly classified images, with darker shades indicating a higher number of correct predictions.\n",
    "- **Off-diagonal cells**: Represent misclassifications; lighter colors indicate fewer errors.\n",
    "\n",
    "The color intensity provides a visual guide to quickly identify areas where the model performs well (darker diagonal) and where it struggles (lighter or darker off-diagonal cells).\n",
    "\n",
    "**Discussion Questions:**\n",
    "\n",
    "- Which classes does the model classify most accurately?\n",
    "- Are there specific classes that the model tends to confuse?\n",
    "- How can you use this confusion matrix to improve your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cc421b-7e91-45ad-9148-9fba1d73c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for the original TF model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_confusion_matrix(input_model, name):\n",
    "    y_pred = input_model.predict(x_test)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_true_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "    \n",
    "    print(f\"Confusion Matrix {name}:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # (Optional) Visualize the confusion matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "show_confusion_matrix(model, \"Small Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b50752-bd99-4e86-bc3d-d1e268b7f7f0",
   "metadata": {},
   "source": [
    "\n",
    "### **Task 3: Teacher Model Interpretation**\n",
    "\n",
    "This cell loads the pre-trained **teacher model** from the file `teacher.h5`. The teacher model is usually a larger and more complex model trained earlier, often achieving higher accuracy.\n",
    "\n",
    "Here:\n",
    "\n",
    "- **Loading** the model from the file.\n",
    "- Printing the model's **summary**, which provides a detailed overview of its layers, output shapes, and total number of parameters.\n",
    "\n",
    "The teacher model will be used later as a source of knowledge for training smaller, efficient student models.\n",
    "\n",
    "**Questions to Consider:**\n",
    "\n",
    "- What role does the teacher model play in knowledge distillation?\n",
    "- Why is the teacher model typically larger and more accurate than the student model?\n",
    "- How many parameters does the teacher model have compared to the student model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d8588-ba5c-440e-a874-1e11b899b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the teacher model from 'teacher.h5'\n",
    "teacher_model = tf.keras.models.load_model(\"teacher.h5\")\n",
    "\n",
    "# Summarize the teacher model\n",
    "teacher_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bb7aaa-2018-45b4-b16c-19b1834f5dad",
   "metadata": {},
   "source": [
    "\n",
    "### **Evaluate the Teacher Model on the Test Set**\n",
    "\n",
    "In this cell, we evaluate the **teacher model**'s performance on the **CIFAR-10 test dataset** to determine its accuracy. The teacher model typically serves as the performance baseline due to its higher complexity and accuracy.\n",
    "\n",
    "The evaluation provides:\n",
    "\n",
    "- **Loss**: A measure of how well the model predictions match the actual labels.\n",
    "- **Accuracy**: The percentage of correctly classified test images.\n",
    "\n",
    "**Discussion Questions:**\n",
    "\n",
    "- How does the teacher model’s accuracy compare with the student model's accuracy?\n",
    "- Why might the teacher model be more accurate?\n",
    "- Is the teacher model accuracy significantly higher, justifying the increased model complexity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ce656-4d2c-4ccb-bf59-eb702685cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the teacher model on the same test set\n",
    "teacher_loss, teacher_acc = teacher_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Teacher model test accuracy: {teacher_acc:.4f}\")\n",
    "print(f\"Teacher model test loss: {teacher_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f86cbd-affe-4c71-930b-e5b018d7e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(teacher_model, \"Teacher Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc3aa7-91dc-49a7-89d9-a9e4cb6c6c93",
   "metadata": {},
   "source": [
    "##  **Quantize and Evaluate the Teacher Model (INT8)**\n",
    "\n",
    "In this cell, we perform **post-training integer quantization (INT8)** on the teacher model, following a similar approach as previously used. The purpose of quantizing the teacher model is to:\n",
    "\n",
    "- Significantly reduce model size and memory usage.\n",
    "- Improve inference efficiency, enabling deployment on devices with limited resources.\n",
    "\n",
    "**Steps taken:**\n",
    "- Convert the teacher model from floating-point (FP32) to integer (INT8) precision.\n",
    "- Save the quantized TFLite model as `\"quantized_teacher_model.tflite\"`.\n",
    "- Evaluate the quantized model's accuracy on the CIFAR-10 test set to understand how quantization affects performance.\n",
    "\n",
    "**Points for Discussion:**\n",
    "- How does the quantized teacher model's accuracy compare with its FP32 version?\n",
    "- Why is it important to quantify accuracy loss due to quantization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ae2d25-5dc9-4f74-9348-18c4fb080d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Quantize the Teacher Model & Compare Models\n",
    "# ======================\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(teacher_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_teacher_model = converter.convert()\n",
    "\n",
    "quantized_teacher_model_path = \"quantized_teacher_model.tflite\"\n",
    "with open(quantized_teacher_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_teacher_model)\n",
    "\n",
    "print(\"Quantized Teacher TFLite model saved to:\", quantized_teacher_model_path)\n",
    "\n",
    "teacher_quant_acc = evaluate_tflite_model(quantized_teacher_model_path, x_test, y_test)\n",
    "print(f\"Quantized (INT8) Teacher model test accuracy: {teacher_quant_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5af09e-7b59-488a-8d9e-5e633ccf7423",
   "metadata": {},
   "source": [
    "## **Comparing Model Performance: Teacher vs Small Model**\n",
    "\n",
    "This cell presents a detailed **side-by-side comparison** of both the Teacher and Small models, highlighting:\n",
    "\n",
    "- **Accuracy**:\n",
    "  - Comparison of accuracy between original floating-point (FP32) and quantized integer (INT8) models.\n",
    "  - Calculation of accuracy differences helps illustrate the impact quantization has on each model.\n",
    "\n",
    "- **Model Complexity (Parameters)**:\n",
    "  - Number of parameters for both the Teacher and Small models.\n",
    "  - Difference in parameters indicates how much smaller or simpler the Small model is compared to the Teacher.\n",
    "\n",
    "This comparison is crucial for understanding the trade-offs between model size, computational requirements, and accuracy when choosing a suitable model for deployment.\n",
    "\n",
    "**Discussion Points:**\n",
    "- How significant is the accuracy loss from FP32 to INT8 quantization for each model?\n",
    "- In practical scenarios, under what conditions might you prefer the Small model over the Teacher model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0409515-7582-4140-a533-17fa0e08ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Comparison =====\")\n",
    "# Teacher FP32\n",
    "print(f\"Teacher model accuracy (FP32):           {teacher_acc:.4f}\")\n",
    "# Teacher INT8\n",
    "print(f\"Teacher model accuracy (INT8):           {teacher_quant_acc:.4f}\")\n",
    "print(f\"Accuracy diff (Teacher FP32 - Teacher INT8): {teacher_acc - teacher_quant_acc:.4f}\")\n",
    "\n",
    "# Small FP32\n",
    "print(f\"\\nSmall model accuracy (FP32):             {small_model_acc:.4f}\")\n",
    "# Small INT8\n",
    "print(f\"Small model accuracy (INT8):             {small_quantized_acc:.4f}\")\n",
    "print(f\"Accuracy diff (Small FP32 - Small INT8): {small_model_acc - small_quantized_acc:.4f}\")\n",
    "\n",
    "# Number of parameters\n",
    "small_model = model\n",
    "teacher_params = teacher_model.count_params()\n",
    "small_params   = model.count_params()\n",
    "\n",
    "print(f\"\\nTeacher model parameters:   {teacher_params}\")\n",
    "print(f\"Small model parameters:     {small_params}\")\n",
    "print(f\"Parameter difference:       {teacher_params - small_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc23c64-c8b7-434e-90bc-9945f0f36242",
   "metadata": {},
   "source": [
    "## **Calculate and Compare Memory Footprint of Models (FP32 vs INT8)**\n",
    "\n",
    "In this cell, we calculate the approximate **memory requirements** of both the Teacher and Small models in two data formats:\n",
    "\n",
    "- **FP32 (float32)**: Each parameter occupies **4 bytes**.\n",
    "- **INT8 (integer quantized)**: Each parameter occupies **1 byte**.\n",
    "\n",
    "By comparing these memory footprints, we clearly see the benefit of quantizing models from FP32 to INT8, significantly reducing storage and memory usage.\n",
    "\n",
    "**Key Insights:**\n",
    "- Quantized models typically use **4x less memory** than their FP32 counterparts.\n",
    "- This reduction can lead to faster inference speeds and lower power consumption, particularly on devices with limited resources.\n",
    "\n",
    "**Questions to Explore:**\n",
    "- How critical is memory footprint for your deployment scenario? **Please check your microcontroller's datasheet** and try to understand if any of these models can fit into your microcontroller memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736a6d1-7d8e-45ce-bc11-931049e8e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Calculate Memory Footprint for FP32 & INT8\n",
    "# =============================\n",
    "\n",
    "BYTES_PER_FP32 = 4\n",
    "BYTES_PER_INT8 = 1\n",
    "BYTES_PER_KB   = 1024\n",
    "BYTES_PER_MB   = 1024 * 1024\n",
    "\n",
    "def calc_memory_usage(num_params, bytes_per_param):\n",
    "    total_bytes = num_params * bytes_per_param\n",
    "    kb = total_bytes / BYTES_PER_KB\n",
    "    mb = total_bytes / BYTES_PER_MB\n",
    "    return total_bytes, kb, mb\n",
    "\n",
    "# Teacher model: FP32\n",
    "teacher_fp32_bytes, teacher_fp32_kb, teacher_fp32_mb = calc_memory_usage(teacher_params, BYTES_PER_FP32)\n",
    "\n",
    "# Teacher model: INT8\n",
    "teacher_int8_bytes, teacher_int8_kb, teacher_int8_mb = calc_memory_usage(teacher_params, BYTES_PER_INT8)\n",
    "\n",
    "# Small model: FP32\n",
    "small_fp32_bytes, small_fp32_kb, small_fp32_mb = calc_memory_usage(small_params, BYTES_PER_FP32)\n",
    "\n",
    "# Small model: INT8\n",
    "small_int8_bytes, small_int8_kb, small_int8_mb = calc_memory_usage(small_params, BYTES_PER_INT8)\n",
    "\n",
    "print(\"===== Memory Footprint (Approximate) =====\")\n",
    "print(\"\\nTeacher Model:\")\n",
    "print(f\"  FP32: {teacher_fp32_bytes} bytes | {teacher_fp32_kb:.2f} KB | {teacher_fp32_mb:.2f} MB\")\n",
    "print(f\"  INT8: {teacher_int8_bytes} bytes | {teacher_int8_kb:.2f} KB | {teacher_int8_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nSmall Model:\")\n",
    "print(f\"  FP32: {small_fp32_bytes} bytes | {small_fp32_kb:.2f} KB | {small_fp32_mb:.2f} MB\")\n",
    "print(f\"  INT8: {small_int8_bytes} bytes | {small_int8_kb:.2f} KB | {small_int8_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1a995-1b37-4085-9203-8572df533717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the visualized model\n",
    "def show_model_visualization(model, title):\n",
    "    img = visualkeras.layered_view(model, legend=True)  # Generate visualization\n",
    "    img_path = f\"{title}.png\"\n",
    "    img.save(img_path)  # Save as an image\n",
    "    display.display(Image.open(img_path))  # Display in Jupyter Notebook\n",
    "\n",
    "# Visualizing models\n",
    "print(\"Teacher Model Architecture:\\r\\n\")\n",
    "show_model_visualization(teacher_model, \"teacher_model\")\n",
    "\n",
    "print(\"\\r\\n\\n\")\n",
    "\n",
    "print(\"\\nSmall Model Architecture:\\r\\n\")\n",
    "show_model_visualization(small_model, \"small_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdd194-3dff-4cd3-b584-d94267da2352",
   "metadata": {},
   "source": [
    "## **Task 4: Knowledge Distillation**\n",
    "\n",
    "This cell introduces a custom class, `Distiller`, that implements the **knowledge distillation** training approach using Keras. Knowledge distillation aims to transfer knowledge from a large, accurate **teacher model** to a smaller, more efficient **student model**.\n",
    "\n",
    "### **How the Distiller Class Works:**\n",
    "\n",
    "- **Initialization:**\n",
    "  - Accepts two models:\n",
    "    - A pre-trained **teacher model** (larger, high-performance).\n",
    "    - A smaller **student model** (simpler, faster).\n",
    "    \n",
    "- **Compilation Parameters:**\n",
    "  - **Optimizer**: Used to update the student model's weights.\n",
    "  - **Metrics**: Metrics for evaluating the student model (e.g., accuracy).\n",
    "  - **Student loss function** (`student_loss_fn`): Measures how well the student predictions match the true labels (hard labels).\n",
    "  - **Distillation loss function** (`distillation_loss_fn`): Measures the difference between teacher and student predictions using softened probabilities (soft labels).\n",
    "  - **Alpha (`α`)**: Balances the contribution of the student loss and distillation loss:\n",
    "    - `loss = α * student_loss + (1 - α) * distillation_loss`\n",
    "  - **Temperature (`T`)**: Used to soften probability distributions, making it easier for the student model to learn from the teacher's predictions:\n",
    "    - Higher temperatures produce softer distributions, emphasizing relationships among classes.\n",
    "\n",
    "- **Custom `train_step`:**\n",
    "  - Performs a forward pass with both teacher (no gradient) and student (gradient computed).\n",
    "  - Calculates two losses:\n",
    "    - **Student loss**: Measures the student's performance against true labels.\n",
    "    - **Distillation loss**: Encourages the student model to mimic the softened predictions of the teacher model.\n",
    "  - Combines losses using the factor `α`.\n",
    "  - Updates the student model parameters by backpropagating the combined loss.\n",
    "  - Updates and returns training metrics and individual loss values.\n",
    "\n",
    "- **Custom `test_step`:**\n",
    "  - Evaluates student model performance during validation or testing.\n",
    "  - Updates and returns evaluation metrics, including the student loss.\n",
    "\n",
    "### **Important Notes:**\n",
    "- Because the teacher model outputs probabilities (softmax), the implementation re-applies softmax after scaling by temperature. Although not mathematically ideal (logits would be preferred), this practical approach typically performs well.\n",
    "- Multiplying the distillation loss by `T²` (temperature squared) is a standard best practice from the original knowledge distillation paper by Hinton et al.\n",
    "\n",
    "---\n",
    "\n",
    "### **Questions for Further Exploration:**\n",
    "- What happens to the student model’s accuracy when changing the temperature parameter? (Check the mathematical expression)\n",
    "- How sensitive is the knowledge distillation process to the choice of alpha (α)? (Check the mathematical expression)\n",
    "- Could removing the teacher's softmax layer (using logits directly) improve distillation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddcb83-aeaf-4052-a2b0-942a2142d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Distiller for Knowledge Distillation\n",
    "\n",
    "class Distiller(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Custom model that encapsulates knowledge distillation.\n",
    "    \"\"\"\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student.\n",
    "            metrics: Keras metrics for the student’s predictions.\n",
    "            student_loss_fn: Loss function for student outputs (hard labels).\n",
    "            distillation_loss_fn: Loss function between teacher & student soft predictions.\n",
    "            alpha: Weight for the student_loss_fn.\n",
    "            temperature: Temperature for softening logits (teacher & student).\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher in inference mode\n",
    "        teacher_predictions = self.teacher(x, training=False)  \n",
    "        # These are probabilities because teacher has softmax, but we treat them as \"logits\" anyway.\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Hard-label loss: student vs. ground truth\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "            # Soft targets: apply temperature to teacher & student predictions\n",
    "            # Even though teacher_predictions is already a probability distribution,\n",
    "            # we treat it as if it were logits.\n",
    "            teacher_soft = tf.nn.softmax(teacher_predictions / self.temperature, axis=1)\n",
    "            student_soft = tf.nn.softmax(student_predictions / self.temperature, axis=1)\n",
    "\n",
    "            # Distillation loss\n",
    "            distillation_loss = self.distillation_loss_fn(teacher_soft, student_soft)\n",
    "            # Multiply by T^2 (common practice from Hinton’s Distillation paper)\n",
    "            distillation_loss *= (self.temperature ** 2)\n",
    "\n",
    "            # Combine the two losses\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients wrt student\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics (student's accuracy, etc.)\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        # Optionally log individual loss terms\n",
    "        results.update({\n",
    "            \"distillation_loss\": distillation_loss,\n",
    "            \"student_loss\": student_loss\n",
    "        })\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Student forward pass\n",
    "        student_predictions = self.student(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b4279-0bfe-4ad4-918d-c70093456f00",
   "metadata": {},
   "source": [
    "## **Set Up Knowledge Distillation and Model Checkpointing**\n",
    "\n",
    "In this cell, we configure and prepare the **knowledge distillation** process using the previously defined `Distiller` class. We load the **teacher model**, build the student model, and set up training with customizable parameters:\n",
    "\n",
    "- **Alpha (`kd_alpha`)**: Controls the balance between two loss components:\n",
    "  - The **student loss** (accuracy with respect to true labels).\n",
    "  - The **distillation loss** (matching soft predictions from the teacher).\n",
    "\n",
    "- **Temperature (`temperature`)**: Softens the output distributions from the teacher and student. Higher temperatures lead to softer probability distributions, emphasizing inter-class relationships rather than absolute predictions.\n",
    "\n",
    "- A **checkpoint callback** is also defined to save only the best model weights based on validation accuracy, allowing recovery of the best-performing student model.\n",
    "\n",
    "### **Experiment Suggestions:**\n",
    "Try experimenting with different values for the hyperparameters to observe their impact:\n",
    "\n",
    "- **Alpha (`kd_alpha`)**: Experiment with values like **0, 0.1, 0.3, 0.5, 0.7, 1**.\n",
    "  - Observe how changing `alpha` affects the balance between accuracy (true labels) and learning from the teacher's predictions.\n",
    "\n",
    "- **Temperature**: Experiment with temperatures like **2, 4, 5, 10, 50, 100, 200**.\n",
    "  - See how different temperatures influence model accuracy and generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9e82-7877-4ce1-a03b-27c50f1bd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = tf.keras.models.load_model(\"teacher.h5\", compile=False)\n",
    "print(\"Teacher model loaded successfully.\")\n",
    "\n",
    "student = build_cifar10_model()\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "\n",
    "#########################################\n",
    "# Knowledge Distillation Hyper Parameters\n",
    "\n",
    "kd_alpha = 0.5\n",
    "kd_temperature = 200\n",
    "\n",
    "#########################################\n",
    "\n",
    "\n",
    "\n",
    "distiller.compile(\n",
    "    optimizer=get_optimizer_for_platform(learning_rate=5e-3),\n",
    "    metrics=['accuracy'],\n",
    "    student_loss_fn=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "    alpha=kd_alpha,       # weight for the student’s own CE loss\n",
    "    temperature=kd_temperature  # temperature for softening\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint_path = \"kd_checkpoint.h5\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_accuracy',  # monitors student's val_accuracy\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3a2fa3-fafe-4136-8f82-4dbcff950a50",
   "metadata": {},
   "source": [
    "## **Train the Student Model with Knowledge Distillation**\n",
    "\n",
    "This cell starts the training of our **student model** using the **knowledge distillation** approach. During this training:\n",
    "\n",
    "- The student model learns not only from the true labels but also from the softened predictions of the **teacher model**.\n",
    "- Both the **student loss** (true label accuracy) and the **distillation loss** (teacher-student similarity) are optimized simultaneously, balanced according to the previously set `alpha` parameter.\n",
    "- The model performance is monitored on the **validation set**, and the best-performing weights (based on highest validation accuracy) are automatically saved to the specified checkpoint file (`kd_checkpoint.h5`).\n",
    "\n",
    "**Experiment Recommendations:**\n",
    "\n",
    "- Observe how quickly the validation accuracy improves compared to traditional training without distillation.\n",
    "- Note any differences in training behavior when changing the values of `alpha` and `temperature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a001c7-5563-46c9-90dd-363de1641631",
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=train_epochs,\n",
    "    batch_size=train_batch_size,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0f29a-d6b5-4e9a-b9f2-f94096e4a657",
   "metadata": {},
   "source": [
    "## **Evaluate the Distilled Student Model and Apply Quantization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4345cd-c055-431a-a5ca-185554e2917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distiller.student.compile(\n",
    "    optimizer=get_optimizer_for_platform(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "loss, acc = distiller.student.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Best checkpoint (student) test accuracy after knowledge distillation: {acc:.4f}\")\n",
    "\n",
    "student = distiller.student\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(student)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the quantized student model\n",
    "quantized_model_path = \"quantized_model.tflite\"\n",
    "with open(quantized_model_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"Quantized TFLite student model saved to:\", quantized_model_path)\n",
    "\n",
    "quant_acc = evaluate_tflite_model(quantized_model_path, x_test, y_test)\n",
    "print(f\"Quantized TFLite student model test accuracy: {quant_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228992f2-9748-4588-b96e-7cb883ced0fc",
   "metadata": {},
   "source": [
    "## **Task 5: Evaluating the Final Knowledge-Distilled Model (+100 epochs, Pre-trained)**\n",
    "\n",
    "Since training a high-performing student model with knowledge distillation often requires extensive training (e.g., 100+ epochs) and takes considerable time, we've provided a pre-trained **final knowledge-distilled model** (`knowledge_distilled_model_final.h5`) for you to evaluate directly.\n",
    "\n",
    "### **Steps in this cell:**\n",
    "\n",
    "1. **Load and Compile the Pre-trained KD Model:**\n",
    "\n",
    "2. **Evaluate the Pre-trained KD Model (FP32):**\n",
    "\n",
    "3. **Quantize the Final KD Model (INT8):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff77c5-b259-41ca-9243-fa756b40fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_kd_model = tf.keras.models.load_model(\n",
    "    \"knowledge_distilled_model_final.h5\", \n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# Compile with a standard config\n",
    "final_kd_model.compile(\n",
    "    optimizer=get_optimizer_for_platform(learning_rate=1e-3),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Evaluate final KD model (FP32)\n",
    "final_kd_loss, final_kd_acc = final_kd_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Final KD model (FP32) test accuracy: {final_kd_acc:.4f}\")\n",
    "print(f\"Final KD model (FP32) test loss: {final_kd_loss:.4f}\")\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_kd_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_final_kd_model = converter.convert()\n",
    "quantized_final_kd_path = \"quantized_final_kd_model.tflite\"\n",
    "with open(quantized_final_kd_path, \"wb\") as f:\n",
    "    f.write(tflite_quant_final_kd_model)\n",
    "\n",
    "print(\"Quantized final KD model saved to:\", quantized_final_kd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81595a3f-97c6-4c72-bb0d-73b19afca2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the quantized final KD model\n",
    "quant_final_kd_acc = evaluate_tflite_model(quantized_final_kd_path, x_test, y_test)\n",
    "print(f\"Final KD model (INT8) test accuracy: {quant_final_kd_acc:.4f}\")\n",
    "\n",
    "\n",
    "final_kd_params = final_kd_model.count_params() \n",
    "\n",
    "show_confusion_matrix(final_kd_model, \"Final Knowledge-Distilled Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2321a-6c22-4bfa-91a7-831e6c4bb023",
   "metadata": {},
   "source": [
    "## **Model Comparison: Parameters vs. Accuracy (INT8 Quantized)**\n",
    "\n",
    "This final comparison clearly illustrates how the three different models—**Teacher**, **Small**, and **Final KD (Knowledge-Distilled)**—perform when fully quantized (INT8):\n",
    "\n",
    "- The scatter plot visually represents the relationship between the number of parameters (model complexity) and the accuracy on the test set.  \n",
    "  - **X-axis:** Number of parameters (model complexity).\n",
    "  - **Y-axis:** Test accuracy (performance).\n",
    "\n",
    "- Below the plot, the exact numerical accuracies and sizes (in kilobytes) of each quantized INT8 model are displayed for precise comparison.\n",
    "\n",
    "**Insights from this comparison:**\n",
    "- You can clearly observe the trade-offs between **model complexity**, **memory usage**, and **model accuracy**.\n",
    "- This helps identify which model offers the best balance between performance and efficiency.\n",
    "\n",
    "**Discussion Questions:**\n",
    "- Which model provides the best trade-off between size and accuracy?\n",
    "- How does knowledge distillation affect the accuracy and size compared to the original models?\n",
    "- In practical deployment scenarios, which model would you choose, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d130fb-c420-4f7f-8c14-282f5740b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names   = [\"Teacher\", \"Small Model\", \"Final KD Model\"]\n",
    "model_params  = [teacher_params,    small_params,    final_kd_params]\n",
    "model_accs    = [teacher_acc,       small_model_acc,       quant_final_kd_acc]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(model_params, model_accs)\n",
    "\n",
    "\n",
    "for i, name in enumerate(model_names):\n",
    "    plt.annotate(name, (model_params[i], model_accs[i]),\n",
    "                 xytext=(5, 3), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Model Size (Params) vs. Accuracy\")\n",
    "plt.xlabel(\"Number of Parameters\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n===== Comparison =====\")\n",
    "\n",
    "print(f\"Teacher model accuracy (INT8):\\t{teacher_quant_acc:.4f}\")\n",
    "print(f\"Small model accuracy (INT8):\\t{small_quantized_acc:.4f}\")\n",
    "print(f\"KD model accuracy (INT8):\\t{quant_final_kd_acc:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nTeacher model parameters:\\t{teacher_params}\\t({calc_memory_usage(teacher_params, BYTES_PER_INT8)[1]:.4f} KB)\")\n",
    "print(f\"Small model parameters:\\t\\t{small_params}\\t({calc_memory_usage(small_params, BYTES_PER_INT8)[1]:.4f} KB)\")\n",
    "print(f\"KD model parameters:\\t\\t{final_kd_params}\\t({calc_memory_usage(final_kd_params, BYTES_PER_INT8)[1]:.4f} KB)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b3ffd3-ad5c-40f0-8f01-6ba88618610b",
   "metadata": {},
   "source": [
    "## **Inference Speed Comparison: Teacher vs Knowledge-Distilled Model**\n",
    "\n",
    "This cell measures and compares the **inference speed** of two trained models:\n",
    "\n",
    "The inference time is measured on the entire CIFAR-10 test set, averaged over multiple runs (3 runs by default) to ensure accuracy in timing.\n",
    "\n",
    "### **Metrics Calculated:**\n",
    "- **Total Inference Time**: How long it takes to run predictions over the entire test set.\n",
    "- **Average Inference Time per Image**: Helpful for practical deployment considerations.\n",
    "\n",
    "### **Questions to Consider:**\n",
    "- Which model offers better performance-to-speed trade-offs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f7480-ae6f-4f5f-ad81-b4b7ea847d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, x_test, num_runs=1):\n",
    "    \"\"\"\n",
    "    Measures how long it takes for 'model' to do inference on x_test.\n",
    "    Optionally runs multiple times (num_runs) and averages to reduce variance.\n",
    "    Returns total time and time per image.\n",
    "    \"\"\"\n",
    "    total_time = 0.0\n",
    "    for _ in range(num_runs):\n",
    "        start = time.time()\n",
    "        _ = model.predict(x_test, verbose=0)\n",
    "        end = time.time()\n",
    "        total_time += (end - start)\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    time_per_image = avg_time / x_test.shape[0]\n",
    "    return avg_time, time_per_image\n",
    "\n",
    "# Example usage:\n",
    "teacher_total, teacher_per_img = measure_inference_time(teacher_model, x_test, num_runs=3)\n",
    "kd_total, kd_per_img = measure_inference_time(final_kd_model, x_test, num_runs=3)\n",
    "\n",
    "print(\"===== Inference Time Comparison =====\")\n",
    "print(f\"Teacher model - total inference time (test set): {teacher_total:.4f} s (avg of 3 runs)\")\n",
    "print(f\"Teacher model - time per inference:             {teacher_per_img:.6f} s\")\n",
    "\n",
    "print(f\"\\nFinal KD model - total inference time (test set): {kd_total:.4f} s (avg of 3 runs)\")\n",
    "print(f\"Final KD model - time per inference:             {kd_per_img:.6f} s\")\n",
    "\n",
    "if teacher_per_img < kd_per_img:\n",
    "    ratio = kd_per_img / teacher_per_img\n",
    "    print(f\"\\nTeacher model is approximately {ratio:.2f}x faster than Final KD model.\")\n",
    "else:\n",
    "    ratio = teacher_per_img / kd_per_img\n",
    "    print(f\"\\nFinal KD model is approximately {ratio:.2f}x faster than Teacher model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0e33-de21-4805-8edd-b3b352e080d2",
   "metadata": {},
   "source": [
    "## **Convert Quantized TFLite Model (.tflite) to C Source Array (.h)**\n",
    "\n",
    "To deploy your trained model on embedded systems, it's often helpful to convert your TensorFlow Lite model into a C/C++ byte array. This format allows direct embedding of the model into microcontroller projects or other embedded applications.\n",
    "\n",
    "In this cell, we convert your previously quantized TFLite model (`quantized_final_kd_model.tflite`) into a C array format (`.h` file).\n",
    "\n",
    "#### After saving the byte array, it may take a bit time to open the saved .h file to see its content. To explore it, using **nano** text editor or using **cat** directly in your terminal may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9215afb-a784-42ec-9b54-e32a5f2674f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your quantized TFLite model\n",
    "tflite_model_path = \"quantized_final_kd_model.tflite\"\n",
    "\n",
    "# Output C array file\n",
    "c_array_filename = \"quantized_final_kd_model.h\"\n",
    "\n",
    "# Convert .tflite to C byte array\n",
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model_content = f.read()\n",
    "\n",
    "hex_array = ', '.join(f'0x{byte:02x}' for byte in tflite_model_content)\n",
    "c_source_code = f\"\"\"\n",
    "#ifndef QUANTIZED_FINAL_KD_MODEL_H\n",
    "#define QUANTIZED_FINAL_KD_MODEL_H\n",
    "\n",
    "const unsigned char quantized_final_kd_model[] = {{\n",
    "    {hex_array}\n",
    "}};\n",
    "const unsigned int quantized_final_kd_model_len = {len(tflite_model_content)};\n",
    "#endif // QUANTIZED_FINAL_KD_MODEL_TFLITE\n",
    "\"\"\"\n",
    "\n",
    "# Save as .h file\n",
    "c_model_path = \"quantized_final_model.h\"\n",
    "with open(c_model_path, \"w\") as f:\n",
    "    f.write(c_source_code)\n",
    "\n",
    "print(f\"C/C++ header file saved as: {c_model_path}\")\n",
    "\n",
    "print(f\"Model size: {len(tflite_model_content)} bytes ({len(tflite_model_content)/1024:.2f} KB)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
